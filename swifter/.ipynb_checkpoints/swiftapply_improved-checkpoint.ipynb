{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from psutil import cpu_count\n",
    "from dask import dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "\n",
    "def pd_apply(df, myfunc, *args, **kwargs):\n",
    "    def wrapped():\n",
    "        if type(df) == pd.DataFrame:\n",
    "            pd.concat([df[c].apply(myfunc, args=args, **kwargs) for c in df.columns], axis=1)\n",
    "        else:\n",
    "            df.apply(myfunc, args=args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def dask_apply(df, npartitions, myfunc, *args, **kwargs):\n",
    "    if type(df) == pd.DataFrame:\n",
    "        tmp = kwargs.pop('meta')\n",
    "        meta = {c: tmp[c].dtype for c in tmp.columns}\n",
    "        try:\n",
    "            return dd.from_pandas(df, npartitions=npartitions).apply(myfunc, *args, **kwargs, axis=1, meta=meta).compute(get=get)\n",
    "        except:\n",
    "            warnings.warn('Dask applymap not working correctly. Concatenating swiftapplies instead.')\n",
    "            return pd.concat([swiftapply(df[c], myfunc, *args, **kwargs) for c in df.columns], axis=1)\n",
    "    else:\n",
    "        meta = kwargs.pop('meta')\n",
    "        try:\n",
    "            return dd.from_pandas(df, npartitions=npartitions).map_partitions(myfunc, *args, **kwargs, meta=meta).compute(get=get)\n",
    "        except:\n",
    "            return dd.from_pandas(df, npartitions=npartitions).map(lambda x: myfunc(x, *args, **kwargs), meta=meta).compute(get=get)\n",
    "        \n",
    "\n",
    "def estimate_and_apply(df, npartitions, dask_threshold, myfunc, *args, **kwargs):\n",
    "    try:\n",
    "        samp = df.iloc[:1000]\n",
    "    except:\n",
    "        samp = df.iloc[:df.shape[0]/10]\n",
    "\n",
    "    wrapped = pd_apply(samp, myfunc, *args, **kwargs)\n",
    "    n_repeats = 3\n",
    "    timed = timeit.timeit(wrapped, number=n_repeats)\n",
    "    samp_proc_est = timed/n_repeats\n",
    "    est_apply_duration = samp_proc_est / len(samp) * df.shape[0]\n",
    "\n",
    "    # Get meta information for dask, and check if output is str \n",
    "    if type(df) == pd.DataFrame:\n",
    "        kwargs['meta'] = pd.concat([df.loc[:2, c].apply(myfunc, args=args, **kwargs) for c in df.columns], axis=1)\n",
    "        str_target = object in kwargs['meta'].dtypes.values\n",
    "    else:\n",
    "        kwargs['meta'] = df.iloc[:2].apply(myfunc, args=args, **kwargs)\n",
    "        str_target = object == kwargs['meta'].dtypes\n",
    "\n",
    "    # if pandas apply takes too long and output is not str, use dask\n",
    "    if (est_apply_duration > dask_threshold) and (not str_target):\n",
    "        return dask_apply(df, npartitions, myfunc, *args, **kwargs)\n",
    "    else:  # use pandas\n",
    "        kwargs.pop('meta')\n",
    "        if type(df) == pd.DataFrame:\n",
    "            return pd.concat([df[c].apply(myfunc, args=args, **kwargs) for c in df.columns], axis=1)\n",
    "        else:\n",
    "            return df.apply(myfunc, args=args, **kwargs)\n",
    "        \n",
    "        \n",
    "def swiftapply(df, myfunc, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Efficiently apply any function to a pandas dataframe or series\n",
    "    in the fastest available manner\n",
    "    :param df: The dataframe or series to apply the function to\n",
    "    :param myfunc: The function you wish to apply\n",
    "    :param args: The positional arguments of the function\n",
    "    :param kwargs: The key word arguments of the function\n",
    "        You can also specify npartitions and dask_threshold\n",
    "        npartitions will affect the speed of dask multiprocessing\n",
    "        dask_threshold is the maximum allowed time (in seconds) for a normal pandas apply\n",
    "            before switching to a dask operation\n",
    "    :return: The new dataframe/series with the function applied as quickly as possible\n",
    "    \"\"\"\n",
    "    if 'npartitions' in kwargs.keys():\n",
    "        npartitions = kwargs.pop('npartitions')\n",
    "    else:\n",
    "        npartitions = cpu_count() * 2\n",
    "    if 'dask_threshold' in kwargs.keys():\n",
    "        dask_threshold = kwargs.pop('dask_threshold')\n",
    "    else:\n",
    "        dask_threshold = 1\n",
    "    \n",
    "    if myfunc is not str:\n",
    "        # If we are manipulating strings, we don't want to try vectorization\n",
    "        if type(df) == pd.DataFrame:\n",
    "            str_source = object in df.dtypes.values\n",
    "        else:\n",
    "            str_source = object == df.dtypes\n",
    "            \n",
    "        if str_source:\n",
    "            return estimate_and_apply(df, npartitions, dask_threshold, myfunc, *args, **kwargs)\n",
    "                          \n",
    "        try:  # try to vectorize\n",
    "            if type(df) == pd.DataFrame:\n",
    "                return pd.concat([pd.Series(myfunc(df[c], *args, **kwargs), name=c) for c in df.columns], axis=1)\n",
    "            else:\n",
    "                return myfunc(df, *args, **kwargs)\n",
    "        except:  # if can't vectorize, estimate time to pandas apply\n",
    "            return estimate_and_apply(df, npartitions, dask_threshold, myfunc, *args, **kwargs)\n",
    "\n",
    "    else:\n",
    "        return df.astype(str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
